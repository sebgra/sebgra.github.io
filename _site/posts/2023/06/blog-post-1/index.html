

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Bioinformatics and AI: A First Step - Home</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Home">
<meta property="og:title" content="Bioinformatics and AI: A First Step">


  <link rel="canonical" href="http://localhost:4000/posts/2023/06/blog-post-1/">
  <meta property="og:url" content="http://localhost:4000/posts/2023/06/blog-post-1/">



  <meta property="og:description" name="description" content="Bioinformatics and AI: A First Step">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2023-06-14T00:00:00+00:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Your Name",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Home Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<!-- favicon from https://commons.wikimedia.org/wiki/File:OOjs_UI_icon_academic-progressive.svg -->
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png"/>
<link rel="icon" type="image/svg+xml" href="http://localhost:4000/images/favicon.svg"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png" sizes="32x32"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-192x192.png" sizes="192x192"/>
<link rel="manifest" href="http://localhost:4000/images/manifest.json"/>
<link rel="icon" href="/images/favicon.ico"/>
<meta name="theme-color" content="#ffffff"/>
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>


<!-- Support for MatJax -->
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg persist"><a href="http://localhost:4000/">Home</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/portfolio/">Portfolio</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/softwares/">Softwares</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/community_involvement/">Community Involvement</a></li>
          
          <li id="theme-toggle" class="masthead__menu-item persist tail">
            <a><i id="theme-icon" class="fa-solid fa-sun" aria-hidden="true" title="toggle theme"></i></a>
          </li>
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/Logo.png" class="author__avatar" alt="Sébastien Gradit">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Sébastien Gradit</h3>
    
    <p class="author__bio">PhD bioinformatician leveraging AI and machine learning for genomic and multi-omics data insights to advance therapeutics and diagnostics through explainable AI.</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
        <li class="author__desktop"><i class="fa-solid fa-location-dot icon-pad-right" aria-hidden="true"></i>Paris</li>
      
      
        <li class="author__desktop"><i class="fas fa-fw fa-building-columns icon-pad-right" aria-hidden="true"></i>Institut Pasteur</li>
      
      
      
        <li><a href="mailto:sebastiengradit@gmail.com"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
            
      
        <li><a href="https://scholar.google.com/citations?view_op=list_works&hl=fr&user=1676I_AAAAAJ"><i class="ai ai-google-scholar icon-pad-right"></i>Google Scholar</a></li>
      
      
      
      
        <li><a href=" https://orcid.org/0000-0003-2690-8503"><i class="ai ai-orcid ai-fw icon-pad-right"></i>ORCID</a></li>
      
                              
      
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
        <li><a href="https://github.com/sebgra"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>GitHub</a></li>
      
      
        <li><a href="https://kaggle.com/sbastiengradit"><i class="fab fa-fw fa-kaggle icon-pad-right" aria-hidden="true"></i>Kaggle</a></li>
            
      
        <li><a href="https://www.stackoverflow.com/users/10441276"><i class="fab fa-fw fa-stack-overflow icon-pad-right" aria-hidden="true"></i>Stackoverflow</a></li>
            

      <!-- Font Awesome icons / Social media -->
      
      
            
      
                  
                  
      
            
            
      
        <li><a href="https://www.linkedin.com/in/sebastien-gradit"><i class="fab fa-fw fa-linkedin icon-pad-right" aria-hidden="true"></i>LinkedIn</a></li>
            
      
        <li><a href="https://genomic.social/@sebgra"><i class="fab fa-fw fa-mastodon icon-pad-right" aria-hidden="true"></i>Mastodon</a></li>
      
            
                  
            
      
            
            
      
        <li><a href="https://twitter.com/segradit"><i class="fab fa-fw fa-x-twitter icon-pad-right" aria-hidden="true"></i>X (formerly Twitter)</a></li>
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Bioinformatics and AI: A First Step">
    <meta itemprop="description" content="Bioinformatics and AI: A First Step">
    <meta itemprop="datePublished" content="June 14, 2023">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Bioinformatics and AI: A First Step
</h1>
          
            <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  23 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2023-06-14T00:00:00+00:00">June 14, 2023</time></p>
            
        </header>
      

      <section class="page__content" itemprop="text">
        <h1 id="bioinformatics-and-ai-a-first-step">Bioinformatics and AI: A First Step</h1>

<p>The corresponding notebook about the following article can directly be launched through google collab <a target="_blank" href="https://colab.research.google.com/github/bioinfo-fr/data_ia/blob/main/DNA_CNN_promoters.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a></p>

<!-- <div style="text-align: center;">

 ![DNA Illustration](/images/post_images/article_1/dna_illustration.png)
</div> -->

<div style="text-align: center;">
  <img src="/images/post_images/article_1/dna_illustration.png" alt="DNA Illustration" />
  <p>DNA matrix genetics | TheDigitalArtist</p>
</div>

<p><strong>Artificial Intelligence, Machine Learning, Deep Learning: What About the Data Scientist?</strong></p>

<!-- DNA matrix genetics - TheDigitalArtist -->

<p>Artificial Intelligence (AI), Machine Learning, Deep Learning – these terms feel both foreign and familiar at the same time… How do you find your way through this jungle of technical terms?</p>

<p>Let’s start by defining what <strong>AI</strong> is. A basis for science fiction for some, a source of concern for others, we’ll approach AI from a technical perspective. Inspired by biology and cognitive sciences, and built on mathematical foundations, artificial intelligence is defined as a <strong>set of algorithms designed to replicate decisions made by a human being to accomplish a specific task.</strong></p>

<p>“Human” implies perceptual learning, memory organization, and critical reasoning. Indeed, any AI algorithm will require human knowledge, both in the preparation and labeling of data, and in its interpretation.</p>

<hr />

<h2 id="ai-but-for-what-purpose">AI, but for what purpose?</h2>

<p>The tasks that AI can accomplish are as varied as the definition of AI itself, and as diverse as the number of approaches to solve a given problem. The most commonly solved tasks using artificial intelligence include: <strong>classification</strong> (binary, multi-class), <strong>regression</strong>, <strong>image segmentation</strong> (automated identification of different image components), <strong>data denoising</strong>, <strong>object detection</strong>, <strong>natural language processing</strong>, etc. (this list is not exhaustive). For more general information on deep learning and its applications, click <a href="https://en.wikipedia.org/wiki/Deep_learning">here</a>.</p>

<p>Now, let’s roll up our sleeves and tackle our first AI project.</p>

<hr />

<h2 id="defining-the-problem">Defining the Problem</h2>

<p>For this introduction, we’ll address a relatively simple problem using simple data. Today, we’re focusing on the <strong>classification of DNA sequences</strong> to determine whether these sequences are promoters or not (<strong>binary classification</strong>). The idea is to feed a nucleotide sequence into our network and get a 0 or 1 output, representing non-promoter and promoter characteristics of our sequence respectively (0: the sequence is not a promoter, 1: it is). The data will be divided into two files: one containing promoter sequences, the other containing non-promoter sequences. Data available <a href="https://github.com/bioinfo-fr/data_ia">here</a>, the code notebook is available <a href="https://github.com/bioinfo-fr/data_ia/blob/main/DNA_CNN_promoters.ipynb">there</a>.</p>

<h2 id="data-preparation">Data Preparation</h2>

<p>For this project, though not overly complex, we’ll use <strong>Google Colab</strong> to easily access data and benefit from sufficient resources. Of course, you can reproduce this process locally, and perhaps even with better performance.</p>

<p>Let’s start by “mounting” our Drive so we can access the data, define the necessary paths, and import the required libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span>  <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">keras.wrappers.scikit_learn</span> <span class="kn">import</span> <span class="n">KerasClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
 
<span class="c1"># mount google drive
</span><span class="n">drive</span><span class="p">.</span><span class="n">mount</span><span class="p">(</span><span class="s">'/content/drive'</span><span class="p">)</span>
 
<span class="c1"># Paths definitions
</span><span class="n">work_folder</span> <span class="o">=</span> <span class="s">"/content/drive/MyDrive/Promoters_classification/"</span>
<span class="n">non_promoters_set</span> <span class="o">=</span> <span class="n">work_folder</span> <span class="o">+</span> <span class="s">"NonPromoterSequence.txt"</span>
<span class="n">promoter_set</span> <span class="o">=</span> <span class="n">work_folder</span> <span class="o">+</span> <span class="s">"PromoterSequence.txt"</span>
</code></pre></div></div>

<p>The data isn’t in a “standard” format for common libraries, so we’ll need to format it correctly. Note that this is <strong>FASTA-type data</strong>, meaning the separator will be a chevron <code class="language-plaintext highlighter-rouge">&gt;</code>. You can find an article from bioinfo-fr about the FASTA format <a href="link_to_fasta_article">here</a>.</p>

<p>We want to transform this into a <strong>two-column table</strong>: <code class="language-plaintext highlighter-rouge">sequence</code> and <code class="language-plaintext highlighter-rouge">label</code>. Additionally, our data is currently in two separate files that we’ll need to combine. Let’s get that done.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load, sanitize and label non promoter sequences
</span><span class="n">df_non_promoters</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">non_promoters_set</span><span class="p">,</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">'&gt;'</span><span class="p">,</span> <span class="p">)</span>
<span class="n">df_non_promoters</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'Unnamed : 0'</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s">'all'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_non_promoters</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df_non_promoters</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'EP 1 (+) mt:CoI_1 ; range -400 to -100.'</span><span class="p">,</span> <span class="s">'index'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#data cleaning after error found
</span><span class="n">df_non_promoters</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'Unnamed : 0'</span><span class="p">:</span> <span class="s">"sequence"</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df_non_promoters</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># Load, sanitize and label non promoter sequences
</span><span class="n">df_promoters</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">promoter_set</span><span class="p">,</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">'&gt;'</span><span class="p">,</span> <span class="p">)</span>
<span class="n">df_promoters</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'Unnamed : 0'</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s">'all'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_promoters</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df_promoters</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'EP 1 (+) mt:CoI_1 ; range -100 to 200.'</span><span class="p">,</span> <span class="s">'index'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#data cleaning after error found
</span><span class="n">df_promoters</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'Unnamed : 0'</span><span class="p">:</span> <span class="s">"sequence"</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df_promoters</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Concatenate the two frames
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_non_promoters</span><span class="p">,</span> <span class="n">df_promoters</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape of the full dataset : </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">the</span> <span class="n">full</span> <span class="n">dataset</span> <span class="p">:</span> <span class="p">(</span><span class="mi">22600</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>                                             <span class="n">sequence</span>  <span class="n">label</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">0</span>  <span class="n">TAATTACATTATTTTTTTATTTACGAATTTGTTATTCCGCTTTTAT</span><span class="p">...</span>      <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">1</span>  <span class="n">ATTTTTACAAGAACAAGACATTTAACTTTAACTTTATCTTTAGCTT</span><span class="p">...</span>      <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">2</span>  <span class="n">AGAGATAGGTGGGTCTGTAACACTCGAATCAAAAACAATATTAAGA</span><span class="p">...</span>      <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">3</span>  <span class="n">TATGTATATAGAGATAGGCGTTGCCAATAACTTTTGCGTTTTTTGC</span><span class="p">...</span>      <span class="mi">0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">4</span>  <span class="n">AGAAATAATAGCTAGAGCAAAAAACAGCTTAGAACGGCTGATGCTC</span><span class="p">...</span>      <span class="mi">0</span>
</code></pre></div></div>

<p>We now have a two-column table containing <strong>22,600 sequences and their corresponding labels</strong>. Like any good data analysis, it’s essential to <strong>clean this data</strong>. In our case, this means removing sequences that contain nucleotides marked as “N.”</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nb_sequences_to_drop</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rows_indexes_to_drop</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'sequence'</span><span class="p">]):</span>
    <span class="k">if</span> <span class="s">'N'</span> <span class="ow">in</span> <span class="n">seq</span> <span class="p">:</span>
      <span class="n">nb_sequences_to_drop</span> <span class="o">+=</span><span class="mi">1</span>
      <span class="c1"># display(df.loc[df['sequence'] == seq])
</span>      <span class="n">rows_indexes_to_drop</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of sequence to be dropped : </span><span class="si">{</span><span class="n">nb_sequences_to_drop</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Rows to be dropped : </span><span class="si">{</span><span class="n">rows_indexes_to_drop</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape of the data before dropping sequences containing Ns : </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">rows_indexes_to_drop</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape of the data after dropping sequences containing Ns : </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Number</span> <span class="n">of</span> <span class="n">sequences</span> <span class="n">to</span> <span class="n">be</span> <span class="n">dropped</span> <span class="p">:</span> <span class="mi">1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Rows</span> <span class="n">to</span> <span class="n">be</span> <span class="n">dropped</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1822</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">the</span> <span class="n">data</span> <span class="n">before</span> <span class="n">dropping</span> <span class="n">sequences</span> <span class="n">containing</span> <span class="n">Ns</span> <span class="p">:</span> <span class="p">(</span><span class="mi">22600</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">the</span> <span class="n">data</span> <span class="n">after</span> <span class="n">dropping</span> <span class="n">sequences</span> <span class="n">containing</span> <span class="n">Ns</span> <span class="p">:</span> <span class="p">(</span><span class="mi">22598</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>We’re going to build a <strong>convolutional network</strong> based on textual data. Since this type of network can’t directly process text data, we’ll need to change its representation. A classic way to do this is by using a “<strong>one-hot encoding</strong>” strategy.</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/one_hot_encoding.png" alt="One hot encoding" />
  <p>DNA sequences encoding</p>
</div>

<p>The idea is to characterize (for our problem) each base in the sequence using a <strong>quadruplet of values</strong> that are either 0 or 1, with only one value being 1. Thus, the four bases are encoded as: <strong>A: {1, 0, 0, 0}</strong>, <strong>T: {0, 1, 0, 0}</strong>, <strong>C: {0, 0, 1, 0}</strong>, and <strong>G: {0, 0, 0, 1}</strong>.</p>

<p>In our case, since our sequences are 301 nucleotides long, this transformation will result in a matrix of size <strong>4 x 301</strong> for a single sequence. At the dataset level, the dimensions will be <strong>22598 x 301 x 4</strong>. Let’s perform our encoding.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequence</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'sequence'</span><span class="p">])</span>
<span class="n">encoded_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">def</span> <span class="nf">encode_seq</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">Encode</span> <span class="o">=</span> <span class="p">{</span><span class="s">'A'</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s">'T'</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s">'C'</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="s">'G'</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]}</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">Encode</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sequence</span> <span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">encode_seq</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">encoded_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">encoded_list</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" Shape of one-hot encoded sequences : </span><span class="si">{</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">one</span><span class="o">-</span><span class="n">hot</span> <span class="n">encoded</span> <span class="n">sequences</span> <span class="p">:</span> <span class="p">(</span><span class="mi">22598</span><span class="p">,</span> <span class="mi">301</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<p>Our data is now almost ready. The next step is to <strong>separate the transformed sequences from their labels.</strong></p>

<hr />
<h2 id="preparing-training-test-and-validation-datasets">Preparing Training, Test, and Validation Datasets</h2>

<p>Now, it’s necessary to <strong>split our data into three subsets</strong>: training, testing, and validation. The training data will allow the network to adjust its weights based on the error between its prediction (promoter or non-promoter sequence type) and the actual label. The test data will be used to evaluate the performance of the training, and finally, the validation data will give us the model’s performance once it’s trained.</p>

<p>All three datasets must be <strong>independent</strong> of each other to ensure we don’t encounter a sequence example already seen during training. It’s also necessary to ensure (if possible) that <strong>each set has equal proportions of the different classes</strong>. Let’s prepare our datasets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reshape data to allow network processing
</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">301</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Tensor containing one-hot encoded sequences wich are promotors or not
</span><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span> <span class="c1"># Tensor containing labels for each sequence (0 : sequence is nt a promotor, 1 : sequence is a promotor)
</span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">value_counts</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape of tensor containing sequences one-hot encoded </span><span class="si">{</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">n"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Shape of tensor containing labels relatives to the sequences : </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="mi">0</span>    <span class="mi">11299</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">1</span>    <span class="mi">11299</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Name</span> <span class="p">:</span> <span class="n">label</span><span class="p">,</span> <span class="n">dtype</span> <span class="p">:</span> <span class="n">int64</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">tensor</span> <span class="n">containing</span> <span class="n">sequences</span> <span class="n">one</span><span class="o">-</span><span class="n">hot</span> <span class="n">encoded</span> <span class="p">(</span><span class="mi">22598</span><span class="p">,</span> <span class="mi">301</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Shape</span> <span class="n">of</span> <span class="n">tensor</span> <span class="n">containing</span> <span class="n">labels</span> <span class="n">relatives</span> <span class="n">to</span> <span class="n">the</span> <span class="n">sequences</span> <span class="p">:</span> <span class="p">(</span><span class="mi">22598</span><span class="p">,)</span>
</code></pre></div></div>

<p>We need to <strong>ensure that we have the same number of sequences as labels</strong>. Since that’s the case, let’s proceed with <strong>stratified division</strong> of our data.</p>

<hr />
<h2 id="model-construction">Model Construction</h2>

<p>To answer our question, we’re going to build a <strong>shallow convolutional neural network</strong>, with an output layer having only a single neuron. If this neuron is activated, it indicates that the sequence is a promoter type. To learn more about convolutional networks, I recommend <a href="https://www.youtube.com/watch?v=zG_5OtgxfAg">this video</a>.</p>

<hr />
<h2 id="is-learning-an-optimization-problem">Is Learning an Optimization Problem?</h2>

<p>Learning problems, whether deep or machine learning, can be defined as <strong>optimization problems</strong>. This involves constructing a system or model that is initially “naive,” which we then train. In our case, the goal is to make our model increasingly effective by training it to correctly classify promoter sequences from non-promoter sequences. To do this, we’ll repeatedly ask the network to classify a set of examples in batches and evaluate an error. This error will then be used to adjust the coefficients of the convolution kernels until an optimum is reached.</p>

<p>This optimum corresponds to the global minimum of the <strong>cost function</strong> (or Loss), which evaluates the amount of error made by the model in its classification tasks. We’ll then adjust these weights based on the intensity of this error: the higher the error, the more the coefficients will be modified; conversely, the smaller the error, the less the coefficients will be modified. This error is evaluated by calculating the gradient of the error function, or more formally, the partial derivative at each neuron in the network with respect to the neuron. This same error is weighted by the <strong>learning rate</strong>, which scales the strength of the weight modification. Thus, the lower the learning rate, the slower the training, but the more likely it is to converge towards a global optimum.</p>

<hr />
<h2 id="how-to-define-the-quantity-of-error">How to Define the Quantity of Error</h2>

<p>The quantity of error is evaluated by the <strong>cost function (or Loss)</strong>. This function reflects the number of incorrectly predicted classes when evaluating a set of training sequences. Depending on the problems to be solved, cost functions differ, particularly based on the number of possible classes and any existing relationships between these classes.</p>

<p>The quantity of error is evaluated with each prediction of sequence batches. The partial derivative of this error function will then be calculated with respect to each of the network’s parameters, in order to adjust them in the “opposite” direction of this derivative. Thus, when the quantity of errors stabilizes or reaches very low values, the adjustment of parameters is minimal, and learning could conclude.</p>

<p>In the examples below, the cost function used will be the “<strong>Binary Cross-Entropy</strong>,” which accounts for classification error in a two-class optimization problem.</p>

<hr />
<h2 id="you-said-convolution">You Said Convolution?</h2>

<p><strong>Convolution</strong> is a mathematical operation on matrices. It involves “matching” a matrix representing the data with a matrix called a “<strong>convolution kernel</strong>” (or filter). The resulting matrix is the matrix product between the data matrix and the kernel (to which a matrix containing the bias weights of each neuron is sometimes added).</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/convolution_example.png" alt="Convolution example" />
  <p>DNA sequences encoding</p>
</div>

<hr />
<p>Based on the first convolution operation in the previous illustration, we would have:</p>

\[R_{0} = \sum_{i = 0}^{15}w_ix_i\]

<hr />
<p>This convolution operation is performed multiple times by <strong>sliding the kernel (in red)</strong> along the encoded sequence of dimension 301x4. Here, we’ll use “valid” padding, which only allows convolution when the kernel completely “covers” the sequence. This means the number of convolutions, and consequently the size of the output representation, will be limited to 298. Thus, filter f0 will slide with a stride of 1 across the sequence, resulting in 298 positions, leading to as many convolution operations. Ultimately, for this filter, the output dimension will be 298x1.</p>

<p>We then repeat the same operation for the <strong>26 other filters, from f1 to f26</strong>. The output of this convolution layer will therefore be a representation with dimensions of 298x1x27.</p>

<p>This convolution result will then undergo an <strong>activation operation</strong>, which involves finding the image of each element of the resulting matrix (from the convolution) using the activation function (here, <strong>ReLU</strong> for “Rectified Linear Unit”). This operation aims to highlight the most relevant values/elements of the new representation after convolution, which will then be passed to the next layers of the network.</p>

<hr />
<h2 id="pool">Pool!</h2>

<p>Convolutional layers are followed by <strong>pooling layers</strong>. These layers aim to <strong>subsample the results of the convolutional layers</strong> to retain only the pertinent information from this representation, and also to reduce the size of the initial sequence’s representation as it passes through the network.</p>

<p>There are several methods for subsampling convolution results. The general idea is to select a single value from a more or less large set of contiguous values in the post-convolution representation. First, we need to define the <strong>size of this search window</strong>. In the example below, this size will be 3. Then, we choose the function or method to extract a single value from this window.</p>

<p>This choice could be the average of the window values, the median, the minimum, or in our case, the <strong>maximum</strong>.</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/max_pooling.png" alt="Max pooling" />
  <p>Illustration of max pooling</p>
</div>

<hr />
<p>Thus, in our case, we’ll extract values of interest within a window sliding along the convolution result vector, with a window size of 3. We’ll use the <strong>max function</strong>, meaning we’ll only retain the maximum value for a window of 3 contiguous values before repeating the operation for the next three. More formally:</p>

\[R'_0 = max(R_{0}, R_{1}, ..., R_{n})\]

<hr />
<p>Thus, after subsampling with a window of size 3, the dimensions of our <strong>27 post-convolution representations will be divided by 3.</strong></p>

<hr />
<h2 id="the-concept-of-receptive-fields">The Concept of Receptive Fields</h2>

<p>As we mentioned in the introduction, neural networks are inspired by biology. As we know, in the nervous system, neurons are connected in a cascade. Some neurons receive pre-processed information via the axons of their predecessors at the soma. Therefore, for a given neuron, the received signal compiles information transmitted by its predecessors, who themselves may have received condensed information from the perceptual system.</p>

<p>A parallel can be drawn with <strong>convolutional networks</strong>. The first convolutional block (convolution layer + activation + pooling) “sees” the entire input sequence. However, as we’ve just seen, the output of this first block is smaller in dimension than the input sequence, and it no longer carries biological meaning as such. The information transmitted is a <strong>condensed version of the initial sequence.</strong></p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/receptor_field.png" alt="Receptor field" />
  <p>Illustration of receptor field</p>
</div>

<hr />
<p>If we look at the example above, the sequence is of size 14. Performing a convolution with a kernel of size 4 results in an output representation of size 11. Then, we apply subsampling with a window of size 3, yielding an output representation of size 4 from the first convolutional block. In the next convolutional block, convolving this representation with a kernel of size 4 leads to a new representation of size 1, which after pooling with a window of size 3 also gives an output representation of size 1.</p>

<p>The reasoning remains the same as layers succeed each other: the representations decrease in size, and neurons further down a layer “see” an increasingly broader view of the input data. It’s worth noting that once their values are adjusted at the end of training, the <strong>convolution kernels</strong> can then extract <strong>key features</strong> from the input sequence crucial for determining the network’s output class. Furthermore, the deeper the layers are in the network, the more <strong>complex the extracted features</strong> will be.</p>

<hr />
<h2 id="first-attempt">First Attempt</h2>

<p>In this initial experiment, we’ll use <strong>stochastic gradient descent</strong>, which is a basic optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Python</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">"valid"</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'valid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># We define an early stop for training in case of no significant improvement after several phases to avoid overfitting
</span><span class="n">early_stop</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span> <span class="o">=</span> <span class="s">'val_accuracy'</span><span class="p">,</span> <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                           <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span> <span class="p">)</span>

<span class="c1"># We define a monitoring for the model's performance and launch the training
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">115</span><span class="p">)</span>
<span class="c1"># We plot the training results
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"loss"</span><span class="p">],</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_loss"</span><span class="p">],</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">],</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_accuracy"</span><span class="p">],</span> <span class="s">'orange'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">work_folder</span> <span class="o">+</span> <span class="s">"base_model_deep_no_dropout_sgd.png"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>                                   
</code></pre></div></div>

<p>We can observe several things from our learning curves. First, whether in training or validation, neither the loss nor the performance reaches a plateau. It seems that despite 150 training passes (epochs), the optimum hasn’t been reached.</p>

<p><strong>NB</strong>: A pass (or epoch) corresponds to one training session of the model. At the end of the first epoch, all training examples have been seen once; at the end of epoch ‘n’, all these same examples have been seen ‘n’ times by the network.</p>

<p>Furthermore, we can observe that the <strong>validation performance</strong>, which is on examples not seen during training, is worse.</p>

<p>Our model therefore seems <strong>under-trained</strong> on one hand, and shows <strong>difficulty generalizing</strong>.</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/SGD_results.png" alt="SGD results" />
  <p>Results obtained with SGD optimizer</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model final evaluation
</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Untrained model, accuracy : {:5.2f}%"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="mi">71</span><span class="o">/</span><span class="mi">71</span> <span class="o">-</span> <span class="mi">0</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span> <span class="p">:</span> <span class="mf">0.3394</span> <span class="o">-</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">0.8575</span> <span class="o">-</span> <span class="mi">404</span><span class="n">ms</span><span class="o">/</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">6</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Untrained</span> <span class="n">model</span><span class="p">,</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">85.75</span><span class="o">%</span><span class="p">)</span>
</code></pre></div></div>

<p>While the results aren’t perfect, the model achieves <strong>85.75% accuracy</strong>, which is already quite good. Let’s see how we can improve our model’s performance using a different optimizer.</p>

<h2 id="adam-optimizer">ADAM Optimizer</h2>

<p>Let’s switch the <strong>SGD optimizer</strong> for an <strong>ADAM optimizer</strong>. This optimizer is based on the principles of SGD but utilizes the first and second-order moments of the error function’s gradient. This allows the optimizer to have a “memory” of previous training, and thus of the strength of prior modifications to the convolution kernel coefficients.</p>

<p>Furthermore, this type of optimizer incorporates an <strong>adaptive learning rate</strong>, which will allow for significant modifications to the convolution kernel coefficients at the beginning of training, and less forceful adjustments as the training phases progress.</p>

<p>Thus, our model will learn by taking into account more than just the immediate previous training step, and it will refine its learning more precisely over time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_adam</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">"valid"</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model_adam</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">model_adam</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span> <span class="o">=</span> <span class="s">'val_accuracy'</span><span class="p">,</span> <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                           <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span> <span class="p">)</span>


<span class="n">history_adam</span> <span class="o">=</span> <span class="n">model_adam</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">115</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_adam</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history_adam</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"loss"</span><span class="p">],</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_adam</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history_adam</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_loss"</span><span class="p">],</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_adam</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history_adam</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">],</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_adam</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">history_adam</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_accuracy"</span><span class="p">],</span> <span class="s">'orange'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">work_folder</span> <span class="o">+</span> <span class="s">"base_model_deep_no_dropout_adam.png"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p>The ADAM optimizer is known for converging more quickly (reaching the global minimum of the cost function), but on the other hand, it can struggle to generalize.</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/adam_results.png" alt="Adam results" />
  <p>Results obtained with ADAM optimizer</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Final evaluation
</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model_adam</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Untrained model, accuracy : {:5.2f}%"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="mi">71</span><span class="o">/</span><span class="mi">71</span> <span class="o">-</span> <span class="mi">0</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span> <span class="p">:</span> <span class="mf">0.3810</span> <span class="o">-</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">0.8509</span> <span class="o">-</span> <span class="mi">496</span><span class="n">ms</span><span class="o">/</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">7</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="n">Untrained</span> <span class="n">model</span><span class="p">,</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">85.09</span><span class="o">%</span>
</code></pre></div></div>

<p>We can observe that the model’s performance hovers around 85% accuracy. Nevertheless, both the loss and accuracy curves seem to have plateaued, indicating that the training appears to have run its course. However, we note that the validation loss is higher (and respectively, validation accuracy is lower) than during training. Once again, the model generalizes less effectively on unseen examples. Furthermore, we observe an increase in validation loss beyond the 60th epoch. This is a clear sign of overfitting, where the model learns too well on its training data, causing performance to drop when new examples are presented.</p>

<p>Let’s now explore how to remedy this overfitting issue.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_adam_dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">"valid"</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span> <span class="c1"># Add a Dropout layer
</span><span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span> <span class="c1"># Add a Dropout layer
</span><span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPool1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'valid'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span> <span class="c1"># Add a Dropout layer
</span><span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">model_adam_dropout</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="c1"># We define an early stop for training in case of no significant improvement after several phases to avoid overfitting
</span><span class="n">early_stop</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span> <span class="o">=</span> <span class="s">'val_accuracy'</span><span class="p">,</span> <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                           <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span> <span class="p">)</span>

</code></pre></div></div>

<p>It’s important to note that when the model is trained and making predictions, all neurons are connected. Dropout only applies during the learning process.</p>

<div style="text-align: center;">
  <img src="/images/post_images/article_1/dropout_effect.png" alt="Dropout effect" />
  <p>Effects of the droput on learning process</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model final evaluation
</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model_adam_dropout</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">y_validation</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Untrained model, accuracy : {:5.2f}%"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc</span><span class="p">))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="mi">71</span><span class="o">/</span><span class="mi">71</span> <span class="o">-</span> <span class="mi">0</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span> <span class="p">:</span> <span class="mf">0.2720</span> <span class="o">-</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">0.8819</span> <span class="o">-</span> <span class="mi">365</span><span class="n">ms</span><span class="o">/</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">5</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span>
<span class="n">Untrained</span> <span class="n">model</span><span class="p">,</span> <span class="n">accuracy</span> <span class="p">:</span> <span class="mf">88.19</span><span class="o">%</span>
</code></pre></div></div>

<p>We observe, as in the previous example, that both the loss and accuracy curves reach a plateau for both training and validation.</p>

<p>Furthermore, we no longer see a significant divergence between training and validation (in terms of both loss and accuracy) throughout the training process. This suggests that we’ve successfully trained the model without causing overfitting.</p>

<p>It’s worth noting that the final accuracy value is lower with the use of dropout (around 85%) than in the previous experiment (around 90%). However, when evaluating the model, performance actually increased from 85% for the previous model to 88% for this model.</p>

<p>Attempts to make models more generalizable sometimes come with a loss in performance. However, here we seem to have established a model with respectable performance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This concludes a first experiment in applying deep learning to bioinformatics problems. We built a model capable of classifying 301-nucleotide sequences as either promoter or non-promoter.</p>

<p>We’ve seen that numerous <strong>hyperparameters</strong> come into play during model construction and training to make it more robust to new examples and to train it within reasonable timeframes.</p>

<p>It’s important to keep in mind that this network architecture isn’t the only one that can solve this problem, nor is it necessarily the best. Many possibilities exist to refine this model, both in terms of parameterization and architecture.</p>

<p>The complete code is available on this repo: <a href="https://github.com/bioinfo-fr/data_ia/blob/main/DNA_CNN_promoters.ipynb">https://github.com/bioinfo-fr/data_ia/blob/main/DNA_CNN_promoters.ipynb</a></p>

<h2 id="to-go-further-with-convolutional-networks">To Go Further with Convolutional Networks</h2>

<p>Here are some resources on CNNs: <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></p>


        

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#artificial-intelligence" class="page__taxonomy-item" rel="tag">Artificial Intelligence</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#bioinformatics" class="page__taxonomy-item" rel="tag">Bioinformatics</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#tutorial" class="page__taxonomy-item" rel="tag">Tutorial</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://bsky.app/intent/compose?text=http://localhost:4000/posts/2023/06/blog-post-1/" class="btn btn--bluesky" title="Share on Bluesky"><i class="fab fa-bluesky" aria-hidden="true"></i><span> Bluesky</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/2023/06/blog-post-1/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/posts/2023/06/blog-post-1/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>

  <a href="https://x.com/intent/post?text=http://localhost:4000/posts/2023/06/blog-post-1/" class="btn btn--x" title="Share on X"><i class="fab fa-x-twitter" aria-hidden="true"></i><span> X (formerly Twitter)</span></a>
</section>


      


  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="http://localhost:4000/posts/2024/02/blog-post-2/" class="pagination--pager" title="Bioinformatics and AI: CNNs for bioinformatics
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2024/02/blog-post-2/" rel="permalink">Bioinformatics and AI: CNNs for bioinformatics
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  24 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2024-02-16T00:00:00+00:00">February 16, 2024</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><!-- TODO: Set google collab notebook -->

</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        


<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="http://github.com/sebgra"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">
  &copy; 2025 Your Name, Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.<br />
  Site last updated 2025-05-30
</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>








  </body>
</html>

